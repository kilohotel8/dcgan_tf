{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting parameters\n",
    "size = (64, 64)\n",
    "batch_size = 128\n",
    "learning_rate = 0.0002\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n"
     ]
    }
   ],
   "source": [
    "## resize image\n",
    "filelist = os.listdir('img_align_celeba')\n",
    "for i, file in enumerate(filelist):\n",
    "    im = Image.open('img_align_celeba/'+file)\n",
    "    new_im = im.resize(size)\n",
    "    new_im.save('resized_img/file_{}.jpg'.format(i))\n",
    "    if i % 10000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating tfrecord\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "tfrecords_filename = 'celeba.tfrecords'\n",
    "filelist = os.listdir('resized_img')\n",
    "writer = tf.python_io.TFRecordWriter(tfrecords_filename)\n",
    "\n",
    "for img_path in filelist:\n",
    "\n",
    "    img = (np.array(Image.open('resized_img/' + img_path), np.float32) - 127.5)/127.5\n",
    "    img_bytes = img.tostring()\n",
    "    shape = np.shape(img)\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'height': _int64_feature(shape[0]),\n",
    "        'width': _int64_feature(shape[1]),\n",
    "        'image_raw': _bytes_feature(img_bytes)}))\n",
    "    \n",
    "    writer.write(example.SerializeToString())\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generator\n",
    "def generator(z):\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "    \n",
    "        batch_size = np.shape(z)[0]\n",
    "        z_dim = np.shape(z)[1]\n",
    "\n",
    "        z_reshape = tf.keras.layers.Reshape(target_shape=(1, 1, z_dim))(z)\n",
    "\n",
    "        conv1 = tf.keras.layers.Conv2DTranspose(filters=1024, \n",
    "                            kernel_size=4, \n",
    "                            strides=4, \n",
    "                            padding='SAME',\n",
    "                            kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(z_reshape)\n",
    "        conv1_bn = tf.keras.layers.BatchNormalization()(conv1)\n",
    "        conv1_bn_a = tf.keras.layers.ReLU()(conv1_bn)\n",
    "\n",
    "        conv2 = tf.keras.layers.Conv2DTranspose(filters=512, \n",
    "                            kernel_size=4, \n",
    "                            strides=2,\n",
    "                            padding='SAME',\n",
    "                            kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(conv1_bn_a)\n",
    "        conv2_bn = tf.keras.layers.BatchNormalization()(conv2)\n",
    "        conv2_bn_a = tf.keras.layers.ReLU()(conv2_bn)\n",
    "\n",
    "        conv3 = tf.keras.layers.Conv2DTranspose(filters=256, \n",
    "                            kernel_size=4, \n",
    "                            strides=2, \n",
    "                            padding='SAME',\n",
    "                            kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(conv2_bn_a)\n",
    "        conv3_bn = tf.keras.layers.BatchNormalization()(conv3)\n",
    "        conv3_bn_a = tf.keras.layers.ReLU()(conv3_bn)\n",
    "\n",
    "        conv4 = tf.keras.layers.Conv2DTranspose(filters=128, \n",
    "                            kernel_size=4, \n",
    "                            strides=2,\n",
    "                            padding='SAME',\n",
    "                            kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(conv3_bn_a)\n",
    "        conv4_bn = tf.keras.layers.BatchNormalization()(conv4)\n",
    "        conv4_bn_a = tf.keras.layers.ReLU()(conv4_bn)\n",
    "\n",
    "        conv5 = tf.keras.layers.Conv2DTranspose(filters=3, \n",
    "                            kernel_size=4, \n",
    "                            strides=2,\n",
    "                            padding='SAME',\n",
    "                            kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(conv4_bn_a)\n",
    "\n",
    "        x = tf.keras.activations.tanh(conv5)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## discriminator\n",
    "def discriminator(image):\n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "    \n",
    "        conv1 = tf.keras.layers.Conv2D(filters=128,\n",
    "                                strides=4,\n",
    "                                kernel_size=4,\n",
    "                                padding='SAME')(image)\n",
    "        conv1_bn = tf.keras.layers.BatchNormalization()(conv1)\n",
    "        h1 =  tf.keras.layers.LeakyReLU(alpha=0.2)(conv1_bn)\n",
    "\n",
    "        conv2 = tf.keras.layers.Conv2D(filters=256,\n",
    "                                strides=2,\n",
    "                                kernel_size=4,\n",
    "                                padding='SAME')(h1)\n",
    "        conv2_bn = tf.keras.layers.BatchNormalization()(conv2)\n",
    "        h2 =  tf.keras.layers.LeakyReLU(alpha=0.2)(conv2_bn)\n",
    "\n",
    "        conv3 = tf.keras.layers.Conv2D(filters=512,\n",
    "                                strides=2,\n",
    "                                kernel_size=4,\n",
    "                                padding='SAME')(h2)\n",
    "        conv3_bn = tf.keras.layers.BatchNormalization()(conv3)\n",
    "        h3 =  tf.keras.layers.LeakyReLU(alpha=0.2)(conv3_bn)\n",
    "\n",
    "        conv4 = tf.keras.layers.Conv2D(filters=1024,\n",
    "                                strides=2,\n",
    "                                kernel_size=4,\n",
    "                                padding='SAME')(h3)\n",
    "        conv4_bn = tf.keras.layers.BatchNormalization()(conv4)\n",
    "        h4 =  tf.keras.layers.LeakyReLU(alpha=0.2)(conv4_bn)\n",
    "\n",
    "        conv5 = tf.keras.layers.Conv2D(filters=1,\n",
    "                                strides=2,\n",
    "                                kernel_size=4,\n",
    "                                padding='SAME')(h4)\n",
    "\n",
    "        h5 =  tf.keras.layers.LeakyReLU(alpha=0.2)(conv5)\n",
    "\n",
    "        h5_reshape = tf.reshape(h5, [batch_size, 1])\n",
    "\n",
    "        y = tf.keras.activations.sigmoid(h5_reshape)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_loss(logits_real, logits_fake):\n",
    "    D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(logits_real), logits=logits_real))+\\\n",
    "                            tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(logits_fake), logits=logits_fake))\n",
    "    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(logits_fake), logits=logits_fake))\n",
    "    \n",
    "    return D_loss, G_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(shape=[None, 64, 64, 3], dtype=tf.float32)\n",
    "z = tf.random_normal([batch_size, 100],-1,1)\n",
    "G = generator(z)\n",
    "\n",
    "with tf.variable_scope(\"\") as scope:\n",
    "    D_X = discriminator(X)\n",
    "    scope.reuse_variables()\n",
    "    D_G = discriminator(G)\n",
    "\n",
    "D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'discriminator')\n",
    "G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'generator')    \n",
    "\n",
    "\n",
    "D_opt = tf.train.AdamOptimizer(learning_rate, beta1 = 0.5)\n",
    "G_opt = tf.train.AdamOptimizer(learning_rate, beta1 = 0.5)\n",
    "\n",
    "D_loss, G_loss = gan_loss(D_X, D_G)\n",
    "\n",
    "D_train_step = D_opt.minimize(D_loss, var_list=D_vars)\n",
    "G_train_step = G_opt.minimize(G_loss, var_list=G_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'celeba.tfrecords'\n",
    "def parser(example):\n",
    "    features={'height': tf.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.FixedLenFeature([], tf.int64),\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string)}\n",
    "    parsed_feature = tf.parse_single_example(example, features)\n",
    "    height = parsed_feature['height']\n",
    "    width = parsed_feature['width']\n",
    "    image = parsed_feature['image_raw']\n",
    "    return height, width, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epoch):\n",
    "        batch_num = int(202600/batch_size)\n",
    "        D_loss_total = 0\n",
    "        G_loss_total = 0\n",
    "        for batch in range(batch_num):\n",
    "            _, _, image = tf.data.TFRecordDataset(path).map(parser).batch(batch_size).make_one_shot_iterator().get_next()\n",
    "            image_decoded = tf.decode_raw(image, tf.float32)\n",
    "            decded = sess.run(image_decded)\n",
    "            x = np.reshape(decoded, (batch_size, 64, 64, 3))          \n",
    "            _, D_loss_temp = sess.run([D_train_step, D_loss], feed_dict = {X:x})\n",
    "            _, G_loss_temp = sess.run([G_train_step, G_loss])\n",
    "            \n",
    "            D_loss_total += D_loss_temp\n",
    "            G_loss_total += G_loss_temp\n",
    "        \n",
    "        print(\"D_loss: \" + str(D_loss_total/batch_num) + \", G_loss: \" + str(G_loss_total/batch_num))\n",
    "        samples = sess.run(G)\n",
    "        for i in range(10):       \n",
    "            img = samples[i]\n",
    "            img = img.reshape(64, 64)\n",
    "            img = int(img*127.5 + 127.5)\n",
    "            plt.subplot(2, 5, i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAELNJREFUeJzt3V+sHOV5x/HvrwaaNH9kCAuyMPQQyUrhopjkiBBRRQ2EyE2j4AuoQFFlVZZ8QyuiRkqhlSpF6kW4CfSiqmQFmnNBA5SEGqEoieWAqkqV4ThAYnCICXXBsss5tKCkvUhr8vRix2XPenfOnN2Z2Z3z/D7SanfmzOw858w8533f+fO+igjMLJdfm3UAZtY+J75ZQk58s4Sc+GYJOfHNEnLimyXkxDdLaKrEl7RL0suSXpF0d11BmVmzNOkNPJK2AD8FbgZOAs8Cd0TES/WFZ2ZNOG+Kda8DXomIVwEkPQzcAoxN/IsvvjgWFham2KT1HRn4/LGKy5UZ/o4jIz/ysbJt2Tw4ceIEb775ptZbbprEvwx4fWD6JPDxshUWFhZYXl6eYpPWN7hfy/6e6+7/Md+hkR/xvpt7i4uLlZabpo0/6qg6p90gaZ+kZUnLq6urU2zOzOoyTeKfBC4fmN4OnBpeKCL2R8RiRCz2er0pNmfvioGXSl5R8ho0vN7gpuLdV+m2rEumSfxngR2SrpR0AXA78EQ9YZlZkyZu40fEGUl/DHwP2AI8GBEv1haZmTVmmpN7RMR3gO/UFIuZtWSqxLe2lLWhy+7DGFyvbLmq93KMOjcwe1V/S3uXb9k1S8iJb5aQq/qd0EQVu+FKsQZibLhfR1fvN84lvllCTnyzhJz4Zgm5jd9Jk15+a9G4dr1G3BI8zphnhTwUxPRc4psl5MQ3S8hV/cYNX3qbpJ466Z17TZvgrsHSevr4ZkDl37JiU6KOvdJlLvHNEnLimyXkqn4tyiqOdVQiu1gRnSDmqvXvsup8xc128S9aJ5f4Zgk58c0ScuKbJeQ2/qRqefos+0WlIbXcklf2JGO1uwQz7AaX+GYJOfHNEkpZ1a+lVremWjpplT1BnbIJdTQJyi4JJmiBucQ3S8iJb5aQE98soZRt/FqabBOfKNhE141a7FCzuqoN9KH52kT7pYJ1S3xJD0pakXR0YN5Fkg5KOl68X9hsmGZWpypV/W8Au4bm3Q0ciogdwKFi2sw6Yt3Ej4h/Av5zaPYtwFLxeQnYXXNc80l697VmxOmSIagH15GGRpce+lnXrBlCu0ybw2mXDQc+Liat/V3Kdift/jZNmfTk3qURcRqgeL+kvpDMrGmNn9WXtE/SsqTl1dXVpjdnZhVMmvhvSNoGULyvjFswIvZHxGJELPZ6vQk312FrqpDDL6rVSjuv7JecoOJ8ThOp5DvOaWqdbapF9abKYIug5LfpUhNg0sR/AthTfN4DHKgnHDNrQ5XLed8E/gX4iKSTkvYCXwVulnQcuLmYNrOOWPcGnoi4Y8yPbqo5FjNrSco799bawKNYVdvhle9o61rDvonH1so6x6j4d9TYicmG8trIMF+Di1Vaaj74Xn2zhJz4Zgm5qj+xsoc6JhhaqjZNfn8d31dDc2HSB4IafqioS4/5uMQ3S8iJb5aQE98soURt/HEtsLJ+2IfFwFIaMXfcjMo/TGADv//YsfNKlisdV6/ipcMJdWnPusQ3S8iJb5ZQnqp+7ddaKvbD3sAdYpXj6rxJ6vol1lzOK9vsZH9DX84zs7nmxDdLKE9Vv3L1bfxyGjdV9t2T/sxY8zdeU4+u/jddu1qMnL/eV669gjO+W+7o0P50iW+WkBPfLCEnvllCedr4NYiSKWtCjPxYbm3r/Zw2+Ua/bj0dPQxc4psl5MQ3S2gTV/Wb6B9u3KZKHvDo0CWe7qp2z1z5UuN/Wr4Hu7l/XeKbJeTEN0vIiW+W0CZu4w9r8Nkpt+NnrGK/9xo3wTqHx+bbv1WG0Lpc0lOSjkl6UdJdxfyLJB2UdLx4v7D5cM2sDlWq+meAL0XEVcD1wJ2SrgbuBg5FxA7gUDFtZh2wbuJHxOmI+GHx+RfAMeAy4BZgqVhsCdjdVJCTGR7QuMnxqIeHau7SgMmJ1HEInDNEdzdt6OSepAXgWuAwcGlEnIb+PwfgkrqDM7NmVE58Se8HvgV8MSJ+voH19klalrS8uro6SYxmVrNKiS/pfPpJ/1BEfLuY/YakbcXPtwEro9aNiP0RsRgRi71er46YzWxKVc7qC3gAOBYRXxv40RPAnuLzHuBA/eGtp6xt3US7e9z3lZ1DcJu/XUN/77Ht8Y2c9ynbf93ct1Wu498A/CHwY0nPF/P+HPgq8KikvcBrwG3NhGhmdVs38SPinxn/7+ymesMxszZ0/M69Obmj6pz+8cdOWOM2MiTa4GqlHe2P//qx25rv/e579c0ScuKbJdTxqn6LnW2Uff/wAx9zXs1LZdyQZU08WFX2ENCcHRMu8c0ScuKbJeTEN0uo4238sks3VdtUZZd7ph9vz+ZI0x2mdOgwcIlvlpAT3yyhjlf1h1Wsa5Ve1tHo5UYuazaoO8eHS3yzhJz4Zgk58c0S2mRt/IoqX6XrTpstt+48FTcvXOKbJeTEN0soZ1V/TXVwvp+isipK9lnTT+RNYB6OOJf4Zgk58c0SSlrVtzQq16NLHtZac9FgPpoL03KJb5aQE98sISe+WUJu49smN66zzYrr9FesMZ75uGBcZey890h6RtILkl6U9JVi/pWSDks6LukRSRc0H66Z1aFKVf+XwI0RcQ2wE9gl6XrgXuC+iNgBvAXsbS5MM6vTuokfff9VTJ5fvAK4EXismL8E7G4kwsZtZNRUm38loxNHvPs6R8kxsAkPj0on9yRtKUbKXQEOAj8D3o6IM8UiJ4HLmgnRzOpWKfEj4p2I2AlsB64Drhq12Kh1Je2TtCxpeXV1dfJIzaw2G7qcFxFvA08D1wNbJZ29KrAdODVmnf0RsRgRi71eb5pYzawmVc7q9yRtLT6/F/g0cAx4Cri1WGwPcKCpIM2qq3rOZgPndsacMuiyKtfxtwFLkrbQ/0fxaEQ8Kekl4GFJfwU8BzzQYJxmVqN1Ez8ifgRcO2L+q/Tb+2bWMXnu3JvDDhlsI+oY6mzC79iEx4vv1TdLyIlvltAmrup7+KvNpY79V8d3zEOPedNziW+WkBPfLCEnvllCm7iN3822l3XNuEuE8338ucQ3S8iJb5bQJq7ql/HoqjapzXG8uMQ3S8iJb5aQE98soaRt/DJu/9vm5xLfLCEnvllCrupvpo7UbKa61Eh0iW+WkBPfLKGkVf15r4hZF3XpqHKJb5aQE98sISe+WUJJ2/iW06QX3Kqttykv5xVDZT8n6cli+kpJhyUdl/SIpAuaC9PM6rSRqv5d9AfLPOte4L6I2AG8BeytMzAza06lxJe0Hfh94OvFtIAbgceKRZaA3U0EaFafiqPjTrjepN8+C1VL/PuBLwO/KqY/BLwdEWeK6ZPAZTXHZmYNWTfxJX0OWImII4OzRyw68h+dpH2SliUtr66uThimmdWpSol/A/B5SSeAh+lX8e8Htko6e1VgO3Bq1MoRsT8iFiNisdfr1RCymU1r3cSPiHsiYntELAC3Az+IiC8ATwG3FovtAQ40FqWZ1WqaG3j+DPhTSa/Qb/M/UE9IZta0Dd3AExFPA08Xn18Frqs/JDNrmu/cM2vBvN3V53v1zRJy4psl5MQ3S8iJb5aQE98sISe+WUJOfLOEnPhmCTnxzRLynXuW1PCT5c3eTzcPd+sNcolvlpAT3ywhJ75ZQm7jW1LNtrrbPYOwcS7xzRJy4psl5Kq+WU1GdT09r1zimyXkxDdLyFV929Sa7utu3vrSq8olvllCTnyzhJz4Zgm5jW+b2mC7u+xy26Tt8y616wdVSvxiwMxfAO8AZyJiUdJFwCPAAnAC+IOIeKuZMM2sThup6n8qInZGxGIxfTdwKCJ2AIeKaTPrgGna+LcAS8XnJWD39OGYtSOGXtlUTfwAvi/piKR9xbxLI+I0QPF+SRMBmln9qp7cuyEiTkm6BDgo6SdVN1D8o9gHcMUVV0wQopnVrVKJHxGnivcV4HH6w2O/IWkbQPG+Mmbd/RGxGBGLvV6vnqjNbCrrJr6k90n6wNnPwGeAo8ATwJ5isT3AgaaCNKtD5jb9sCpV/UuBxyWdXf7vI+K7kp4FHpW0F3gNuK25MM2sTusmfkS8ClwzYv5/ADc1EZSZNcu37Jol5MQ3S8iJb5aQE98sIT+dZ83TwHNx4Ytp88AlvllCTnyzhFzVt+a5ej93XOKbJeTEN0vIiW+WkBPfLCEnvllCTnyzhJz4Zgk58c0ScuKbJeTEN0vIiW+WkBPfLCEnvllCTnyzhJz4Zgk58c0ScuKbJVQp8SVtlfSYpJ9IOibpE5IuknRQ0vHi/cKmgzWzelQt8f8a+G5E/Bb94bSOAXcDhyJiB3ComDazDqgyWu4HgU8CDwBExP9ExNvALcBSsdgSsLupIM2sXlVK/A8Dq8DfSXpO0teL4bIvjYjTAMX7JQ3GaWY1qpL45wEfBf42Iq4F/psNVOsl7ZO0LGl5dXV1wjDNrE5VEv8kcDIiDhfTj9H/R/CGpG0AxfvKqJUjYn9ELEbEYq/XqyNmM5vSuokfEf8OvC7pI8Wsm4CXgCeAPcW8PcCBRiI0s9pVHVDjT4CHJF0AvAr8Ef1/Go9K2gu8BtzWTIhmVrdKiR8RzwOLI350U73hmFkbfOeeWUJOfLOEnPhmCTnxzRJy4psl5MQ3S8iJb5aQIqK9jUmrwL8BFwNvtrbh0eYhBnAcwxzHWhuN4zcjYt1741tN/P/fqLQcEaNuCEoVg+NwHLOKw1V9s4Sc+GYJzSrx989ou4PmIQZwHMMcx1qNxDGTNr6ZzZar+mYJtZr4knZJelnSK5Ja65VX0oOSViQdHZjXevfgki6X9FTRRfmLku6aRSyS3iPpGUkvFHF8pZh/paTDRRyPFP0vNE7SlqI/xydnFYekE5J+LOl5ScvFvFkcI610Zd9a4kvaAvwN8HvA1cAdkq5uafPfAHYNzZtF9+BngC9FxFXA9cCdxd+g7Vh+CdwYEdcAO4Fdkq4H7gXuK+J4C9jbcBxn3UW/y/azZhXHpyJi58Dls1kcI+10ZR8RrbyATwDfG5i+B7inxe0vAEcHpl8GthWftwEvtxXLQAwHgJtnGQvwG8APgY/Tv1HkvFH7q8Htby8O5huBJwHNKI4TwMVD81rdL8AHgX+lOPfWZBxtVvUvA14fmD5ZzJuVmXYPLmkBuBY4PItYiur18/Q7ST0I/Ax4OyLOFIu0tX/uB74M/KqY/tCM4gjg+5KOSNpXzGt7v7TWlX2bia8R81JeUpD0fuBbwBcj4ueziCEi3omInfRL3OuAq0Yt1mQMkj4HrETEkcHZbcdRuCEiPkq/KXqnpE+2sM1hU3VlvxFtJv5J4PKB6e3AqRa3P6xS9+B1k3Q+/aR/KCK+PctYAKI/KtLT9M85bJV0th/GNvbPDcDnJZ0AHqZf3b9/BnEQEaeK9xXgcfr/DNveL1N1Zb8RbSb+s8CO4oztBcDt9LvonpXWuweXJPpDkR2LiK/NKhZJPUlbi8/vBT5N/yTSU8CtbcUREfdExPaIWKB/PPwgIr7QdhyS3ifpA2c/A58BjtLyfok2u7Jv+qTJ0EmKzwI/pd+e/IsWt/tN4DTwv/T/q+6l35Y8BBwv3i9qIY7foV9t/RHwfPH6bNuxAL8NPFfEcRT4y2L+h4FngFeAfwB+vcV99LvAk7OIo9jeC8XrxbPH5oyOkZ3AcrFv/hG4sIk4fOeeWUK+c88sISe+WUJOfLOEnPhmCTnxzRJy4psl5MQ3S8iJb5bQ/wFga8pxG7EfkwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    _, _, image = tf.data.TFRecordDataset(path).map(parser).batch(batch_size).make_one_shot_iterator().get_next()\n",
    "    image_decoded = tf.decode_raw(image, tf.float32)\n",
    "    decded = sess.run(image_decded)\n",
    "    x = np.reshape(decoded, (batch_size, 64, 64, 3))\n",
    "    plt.imshow(x[0]*127.5 + 127.5)\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
